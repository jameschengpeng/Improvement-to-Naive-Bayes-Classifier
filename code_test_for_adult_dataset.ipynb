{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import comonotonic as cm\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "import utils\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we assume the last column is the label\n",
    "# If the first column is ID's, remove it\n",
    "filename = \"adult.csv\"\n",
    "random_state = 42\n",
    "df = pd.read_csv(\"Datasets/\"+filename)\n",
    "df = df.drop(['education'], axis=1)\n",
    "colnames = [('X'+str(i)) for i in range(df.shape[1]-1)]\n",
    "colnames.append('Y')\n",
    "df.columns = colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning for adult.csv\n",
    "df = df[df.X1 != '?']\n",
    "df = df[df.X6 != '?']\n",
    "# unrankable features\n",
    "encoded_df = utils.encode_df(df, [1,4,5,6,7,8,12,13])\n",
    "encoded_df = encoded_df.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discretize real-valued data\n",
    "df_copy = encoded_df.copy()\n",
    "# continuous variables; categorical rankable features are [3]\n",
    "cont_col = [0,2,9,10,11]\n",
    "num_categories_list = [3,4,5,6,7,8,9,10]\n",
    "# unrankable columns\n",
    "unrankable = [1,4,5,6,7,8,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e23e374f928f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                                \u001b[0mdf_copy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munrankable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_itr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                                                \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manneal_schedule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cluster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                                                                use_mistaken_accuracy_test = False, random_state = random_state)\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/FYP/Improvement-to-Naive-Bayes-Classifier/utils.py\u001b[0m in \u001b[0;36mdetermine_allocation\u001b[0;34m(cont_col, num_categories_list, df, unrankable, colnames, max_itr, temp, anneal_schedule, use_cluster, use_mistaken_accuracy_test, random_state, min_corr)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mold_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmistaken_accuracy_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallocation_book\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munrankable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_corr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mold_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallocation_book\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munrankable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_corr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0maccuracy_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/FYP/Improvement-to-Naive-Bayes-Classifier/utils.py\u001b[0m in \u001b[0;36maccuracy_test\u001b[0;34m(allocation_book, df, unrankable, colnames, min_corr, use_cluster, random_state)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mpure_como_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpure_comonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munrankable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munrankable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mpure_como_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mY_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpure_como_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# use clustered comonotonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m#uncategorized_x = uncategorized_df[colnames[:-1]].to_numpy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/FYP/Improvement-to-Naive-Bayes-Classifier/comonotonic.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x_test)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0my_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0my_predict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my_predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/FYP/Improvement-to-Naive-Bayes-Classifier/comonotonic.py\u001b[0m in \u001b[0;36mpredict_single\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m# deal with a single case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mprob_distribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_prob_dist_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0mpredicted_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_distribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredicted_class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/FYP/Improvement-to-Naive-Bayes-Classifier/comonotonic.py\u001b[0m in \u001b[0;36mget_prob_dist_single\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mprob_distribution_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprob_distribution_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprob_distribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mprob_distribution\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprob_distribution\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_distribution_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mprob_distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "# Here encounters the combinatorial optimization problem\n",
    "# suppose cont_col has size m, num_categories_list has size n, then there are n^m combinations\n",
    "# need to use some combinatorial optimization method to find the optimal combination\n",
    "accuracy_history, allocation_history = utils.determine_allocation(cont_col, num_categories_list,\n",
    "                                                               df_copy, unrankable, colnames, max_itr = 100,\n",
    "                                                               temp = 2, anneal_schedule = 5, use_cluster = False,\n",
    "                                                               use_mistaken_accuracy_test = False, random_state = random_state)\n",
    "plt.plot(list(accuracy_history.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_history, allocation_history = utils.determine_allocation(cont_col, num_categories_list,\n",
    "                                                               df_copy, unrankable, colnames, max_itr = 100,\n",
    "                                                               temp = 2, anneal_schedule = 5, use_cluster = False,\n",
    "                                                               use_mistaken_accuracy_test = True, random_state = random_state)\n",
    "plt.plot(list(accuracy_history.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_history = list(accuracy_history.values())\n",
    "acc_history.index(max(acc_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.accuracy_test(allocation_history[49], df_copy, unrankable, colnames, 0.5, False, random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparation with Naive Bayes\n",
    "# use the original data for Naive Bayes\n",
    "original_X = encoded_df[colnames[:-1]].to_numpy()\n",
    "original_Y = encoded_df[colnames[-1]].to_numpy()\n",
    "original_X_train, original_X_test, original_Y_train, original_Y_test = train_test_split(original_X, original_Y, test_size=0.2, \n",
    "                                                                                        random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7895850284784377"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "model.fit(original_X_train, original_Y_train)\n",
    "pred = model.predict(original_X_test)\n",
    "utils.get_accuracy(pred, original_Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8174125305126119"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# implementation of NB from scratch\n",
    "nb_classifier = cm.naive_bayes(original_X_train, original_Y_train, cont_col)\n",
    "nb_classifier.run()\n",
    "nb_y_predict = nb_classifier.predict(original_X_test)\n",
    "utils.get_accuracy(nb_y_predict, original_Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustered comonotonic\n",
    "df_for_cluster = encoded_df.copy()\n",
    "#continuous variables; categorical rankable features are [3]\n",
    "cluster_accuracy_history, cluster_allocation_history = utils.determine_allocation(cont_col, num_categories_list,\n",
    "                                                               df_for_cluster, unrankable,\n",
    "                                                               colnames, max_itr = 30, temp = 2,\n",
    "                                                               anneal_schedule = 2, use_cluster = True,\n",
    "                                                               use_mistaken_accuracy_test = False, \n",
    "                                                               random_state = random_state, min_corr = 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_acc_history = list(cluster_accuracy_history.values())\n",
    "cluster_acc_history.index(max(cluster_acc_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 10, 2: 10, 9: 6, 10: 10, 11: 10}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_allocation_history[24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8084621643612693"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clustered comonotonic validation\n",
    "allocation_book_cluster = {0: 10, 2: 7, 9: 8, 10: 6, 11: 6}\n",
    "como_cluster_df = encoded_df.copy()\n",
    "uncategorized_df = encoded_df.copy()\n",
    "\n",
    "for col_idx in allocation_book_cluster.keys():\n",
    "    discretized_col = pd.cut(como_cluster_df.iloc[:,col_idx],allocation_book_cluster[col_idx], labels=[i for i in range(allocation_book_cluster[col_idx])])\n",
    "    como_cluster_df['X'+str(col_idx)] = discretized_col\n",
    "X_cluster = como_cluster_df[colnames[:-1]].to_numpy()\n",
    "Y_cluster = como_cluster_df[colnames[-1]].to_numpy()\n",
    "X_train_cluster, X_test_cluster, Y_train_cluster, Y_test_cluster = train_test_split(X_cluster, Y_cluster, \n",
    "                                                                                    test_size=0.2, random_state=random_state)\n",
    "\n",
    "como_cluster_classifier = cm.clustered_comonotonic(X_train_cluster, Y_train_cluster, unrankable, uncategorized_df, \n",
    "                                                   colnames, min_corr = 0.6, random_state = random_state)\n",
    "como_cluster_classifier.run()\n",
    "Y_cluster_pred = como_cluster_classifier.predict(X_test_cluster)\n",
    "utils.get_accuracy(Y_cluster_pred, Y_test_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8314076484947112\n"
     ]
    }
   ],
   "source": [
    "# weighted average of naive bayes and cluster comonotonic\n",
    "weighted_predict_y = []\n",
    "for i in range(len(X_test_cluster)):\n",
    "    prob_dist_cluster_como = como_cluster_classifier.get_prob_dist_single(X_test_cluster[i])\n",
    "    predicted_class, prob_dist_nb = nb_classifier.predict_single(original_X_test[i])\n",
    "    weighted_predict_y.append(utils.weighted_avg(prob_dist_nb, prob_dist_cluster_como, 0.6))\n",
    "#     print(prob_dist_cluster_como)\n",
    "#     print(prob_dist_nb)\n",
    "#     print(\"------\")\n",
    "print(utils.get_accuracy(weighted_predict_y, Y_test_cluster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('anaconda3': conda)",
   "language": "python",
   "name": "python37464bitanaconda3conda0e45134502714cc5a53ed61e6b150427"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
